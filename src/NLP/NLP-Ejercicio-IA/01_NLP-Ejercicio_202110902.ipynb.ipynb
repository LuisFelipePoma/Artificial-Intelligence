{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYdJ6qZIwWnQ"
      },
      "source": [
        "# 06 - Ejercicio: Normalización de textos y Bolsa de Palabras\n",
        "\n",
        "- En el siguiente ejercicio vamos a trabajar con una serie de artículo obtenido de la web \"https://www.elmundotoday.com/\".\n",
        "\n",
        "- Estos artículos se encuentran en un fichero csv **corpus_mundo_today.csv**.\n",
        "\n",
        "- Este CSV esta formado por 3 campos que son:\n",
        "  - Tema\n",
        "  - Título\n",
        "  - Texto\n",
        "- El ejercicio consiste en Normalizar este **_Corpus_** tomando el _título_ y _texto_ como contenido de cada documento y crear 3 **_Bolsa de Palabras_** de la tres formas vistas en el notebbok **\"05_Bag_of_Words_BoW\"**.\n",
        "\n",
        "## 1.- Ejercicios de Normalización:\n",
        "\n",
        "- Dada una lista en la que cada elemento de la misma tiene el contenido (título + texto) de cada documento del corpus se pide:\n",
        "  <span></span><br><br> 1. **Crear una función que devuelva los documentos _Tokenizados_ (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
        "  _ **input**: lista de documentos (lista de Strings).\n",
        "  _ **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "  <span></span><br><br> 2. **Crear una función que elimine los tokens que sean signos de puntuación y _Stop-Words_.**\n",
        "  _ **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "  _ **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "  <span></span><br><br> 3. **Crear una función que transforme cada token a su lema (_Lematización_)**\n",
        "  _ **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "  _ **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "  <span></span><br><br> 4. **Crear una función que elimine todos los tokens que no sean _Nombres_ (NOUN, PROPN), _Verbos_, _Advervios_ o _Adjetivos_.**\n",
        "  _ **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "  _ **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "  <span></span><br><br>  \n",
        "   5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
        "  _ **input**: lista de documentos (lista de Strings).\n",
        "  _ **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
        "\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv6qkBgRwWnX"
      },
      "source": [
        "## 1.- Ejercicios de Normalización:\n",
        "\n",
        "- Leemos el corpus y pasamos los documentos (Título + Texto) a una lista\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "MtvRq_btwWnY"
      },
      "outputs": [],
      "source": [
        "docs_file = \"corpus_mundo_today.csv\"\n",
        "docs_list = pd.read_csv(docs_file, delimiter=\"|\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pd dataframe to list\n",
        "docs_list = docs_list.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filter docs_list\n",
        "docs_list = list(map(lambda x: x[1:][0] + \" \" + x[1:][0], docs_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['El Gobierno español sumará a Junqueras las condenas que no vaya a cumplir Puigdemont El Gobierno español sumará a Junqueras las condenas que no vaya a cumplir Puigdemont',\n",
              " 'Ciudadanos exige la dimisión de Cifuentes el año que viene sin falta, cuando haya elecciones Ciudadanos exige la dimisión de Cifuentes el año que viene sin falta, cuando haya elecciones',\n",
              " 'Mariano Rajoy admite que incluyó la presidencia de España en su curriculum pero no recuerda haber hecho nada Mariano Rajoy admite que incluyó la presidencia de España en su curriculum pero no recuerda haber hecho nada',\n",
              " 'Cristina Cifuentes empieza a referirse a sí misma como “esa señora de la que usted me habla” Cristina Cifuentes empieza a referirse a sí misma como “esa señora de la que usted me habla”',\n",
              " 'La justicia alemana libera a Puigdemont, pide a Cifuentes que enseñe el trabajo de máster y pregunta quién es “M. Rajoy” La justicia alemana libera a Puigdemont, pide a Cifuentes que enseñe el trabajo de máster y pregunta quién es “M. Rajoy”']"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_list[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7A1rAYSwWna"
      },
      "source": [
        "#### 1. **Crear una función que devuelva los documentos _Tokenizados_ (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
        "\n",
        "- **input**: lista de documentos (lista de Strings).\n",
        "- **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "T-4TOCb_wWnb"
      },
      "outputs": [],
      "source": [
        "# importamos word_tokenize desde nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Pasar el texto de la cadena a la palabra tokenize para romper las oraciones\n",
        "def tokenization(docs_list: list):\n",
        "    docs_list = list(map(lambda x: word_tokenize(x), docs_list))\n",
        "\n",
        "    return docs_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJTRkdaNwWnb"
      },
      "source": [
        "#### 2. **Crear una función que elimine los tokens que sean signos de puntuación y _Stop-Words_.**\n",
        "\n",
        "- **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "- **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "u_Zij4h5wWnc"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "def remove_words(docs: list):\n",
        "    # Eliminamos las palabras vacias\n",
        "    a = set(stopwords.words(\"english\"))\n",
        "    docs = list(map(lambda x: [i for i in x if i not in a], docs))\n",
        "\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVZeM62QwWnd"
      },
      "source": [
        "#### 3. **Crear una función que transforme cada token a su lema (_Lematización_)**\n",
        "\n",
        "- **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "- **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "XEtGUMxUwWnd"
      },
      "outputs": [],
      "source": [
        "# Importando la biblioteca Lemmatizer desde nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "def lematization(docs: list):\n",
        "    # Inicializando el lematizador\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # Lemattize docs list\n",
        "    docs = [[lemmatizer.lemmatize(token.lower()) for token in doc] for doc in docs]\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdz8mIcCwWne"
      },
      "source": [
        "#### 4. **Crear una función que elimine todos los tokens que no sean _Nombres_ (NOUN, PROPN), _Verbos_, _Advervios_ o _Adjetivos_.**\n",
        "\n",
        "- **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "- **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "K4v5aGv5wWne"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\pms_l\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def filter_words(docs: list):\n",
        "    # Eliminamos las palabras vacias\n",
        "    new_docs = []\n",
        "    for doc in docs:\n",
        "        new_doc = []\n",
        "        result = nltk.pos_tag(doc)\n",
        "        for token in result:\n",
        "            if (token[1] == 'NN' or token[1] == 'PRP'):\n",
        "                new_doc.append(token[0])\n",
        "        # fitler only NN and PROPN\n",
        "        new_docs.append(new_doc)\n",
        "    return new_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0h29B0IwWne"
      },
      "source": [
        "#### 5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
        "\n",
        "- **input**: lista de documentos (lista de Strings).\n",
        "- **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "4rzYM9jRwWnf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['el', 'gobierno', 'español', 'sumará', 'que', 'vaya', 'cumplir', 'puigdemont', 'el', 'gobierno', 'español', 'sumará', 'que', 'vaya', 'cumplir', 'puigdemont']\n"
          ]
        }
      ],
      "source": [
        "def normalization(docs_list: list):\n",
        "    corpus = tokenization(docs_list)\n",
        "    corpus = remove_words(corpus)\n",
        "    corpus = lematization(corpus)\n",
        "    corpus = filter_words(corpus)\n",
        "    return corpus\n",
        "\n",
        "corpus = normalization(docs_list)\n",
        "print(corpus[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
