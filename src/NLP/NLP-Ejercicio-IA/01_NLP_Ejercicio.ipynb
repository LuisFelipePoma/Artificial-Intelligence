{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYdJ6qZIwWnQ"
      },
      "source": [
        "# 06 - Ejercicio: Normalización de textos y Bolsa de Palabras\n",
        "\n",
        "* En el siguiente ejercicio vamos a trabajar con una serie de artículo obtenido de la web \"https://www.elmundotoday.com/\".\n",
        "\n",
        "\n",
        "* Estos artículos se encuentran en un fichero csv **corpus_mundo_today.csv**.\n",
        "\n",
        "\n",
        "* Este CSV esta formado por 3 campos que son:\n",
        "    - Tema\n",
        "    - Título\n",
        "    - Texto\n",
        "    \n",
        "    \n",
        "* El ejercicio consiste en Normalizar este ***Corpus*** tomando el *título* y *texto* como contenido de cada documento y crear 3 ***Bolsa de Palabras*** de la tres formas vistas en el notebbok **\"05_Bag_of_Words_BoW\"**.\n",
        "\n",
        "\n",
        "## 1.- Ejercicios de Normalización:\n",
        "\n",
        "* Dada una lista en la que cada elemento de la misma tiene el contenido (título + texto) de cada documento del corpus se pide:\n",
        "<span></span><br><br>\n",
        "    1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
        "        * **input**: lista de documentos (lista de Strings).\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "<span></span><br><br>\n",
        "    2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
        "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "<span></span><br><br>\n",
        "    3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
        "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "<span></span><br><br>\n",
        "    4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
        "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        " <span></span><br><br>       \n",
        "    5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
        "        * **input**: lista de documentos (lista de Strings).\n",
        "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
        "\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv6qkBgRwWnX"
      },
      "source": [
        "## 1.- Ejercicios de Normalización:\n",
        "\n",
        "* Leemos el corpus y pasamos los documentos (Título + Texto) a una lista"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtvRq_btwWnY"
      },
      "outputs": [],
      "source": [
        "docs_file = 'corpus_mundo_today.csv'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7A1rAYSwWna"
      },
      "source": [
        "#### 1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
        "\n",
        "* **input**: lista de documentos (lista de Strings).\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-4TOCb_wWnb"
      },
      "outputs": [],
      "source": [
        "def tokenization(docs_list):\n",
        "    # TODO\n",
        "    return docs_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJTRkdaNwWnb"
      },
      "source": [
        "#### 2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
        "\n",
        "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_Zij4h5wWnc"
      },
      "outputs": [],
      "source": [
        "def remove_words(docs):\n",
        "    # TODO\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVZeM62QwWnd"
      },
      "source": [
        "#### 3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
        "\n",
        "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEtGUMxUwWnd"
      },
      "outputs": [],
      "source": [
        "def lematization(docs):\n",
        "    # TODO\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdz8mIcCwWne"
      },
      "source": [
        "#### 4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
        "\n",
        "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4v5aGv5wWne"
      },
      "outputs": [],
      "source": [
        "def filter_words(docs):\n",
        "    # TODO\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0h29B0IwWne"
      },
      "source": [
        "#### 5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
        "\n",
        "* **input**: lista de documentos (lista de Strings).\n",
        "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rzYM9jRwWnf"
      },
      "outputs": [],
      "source": [
        "def normalization(docs_list):\n",
        "    corpus = tokenization(docs_list)\n",
        "    corpus = remove_words(corpus)\n",
        "    corpus = lematization(corpus)\n",
        "    corpus = filter_words(corpus)\n",
        "    return corpus\n",
        "\n",
        "corpus = normalization(docs_list)\n",
        "print(corpus[0])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}